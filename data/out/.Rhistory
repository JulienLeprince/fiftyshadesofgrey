# Saving the current best model from this iteration
best_fit_new <- best_fit
# Comparing this model with the previous one
load(paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
## Calculate lambda
chisqStat <- -2 * (max(best_fit$loglik) - max(best_fit_new$loglik))
## If this gives a p-value smaller than confidence limit, i.e. 5\%, then the
## new model is significantly better than the previous one
prmDiff <- best_fit_new$model$NPARAM - best_fit$model$NPARAM
## Calculating the p-value of the test
p_val <- 1 - pchisq(chisqStat, prmDiff)
if (p_val < 0.05) { # If the p-value shows significance - we save the new model
# New model is significantly better than the last
# Saving best model
best_newpath <- c(best_fit$best_path, best_model_name)
# Saving best nCPBES
old_nCPBES <- best_fit$nCPBES
best_fit <- best_fit_new  # changing the naming convention of the best_fit model
# Save path information
best_fit$best_path <- best_newpath
# Calculate the one-step predictions of the state (i.e. the residuals)
tmp <- predict(best_fit)[[1]]
# Calculate the residuals and put them with the data in a data.frame X
X$residuals <- X$yTi - tmp$output$pred$yTi
# nCPBES calculation
nCPBES <- nCPBE_calc(X$residuals)
nCPBES <- tail(nCPBES, n=1)
# Save nCPBES information
best_fit$nCPBES <- c(old_nCPBES, nCPBES)
save(best_fit, file=paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
# Update the model list to loop over for next iteration
ite <- best_model_name
} else {
# New model shows no significant improvement of the previous model
# We can terminate the model iteration
while_condition <- FALSE
return(NA)
}
}
} # End of while loop
list_nofit
} # End of [uuid] loop
stopCluster(cl)
all_list_nofit
### Parallel Looping over building input files
n.cores <- parallel::detectCores() - 1
#create the cluster
cl <- parallel::makeCluster(
n.cores,
type = "PSOCK", #FORK (highly efficient - the main environment doesn’t have to be copied - only available on UNIX), PSOCK (environment of the director needs to be copied - available for both UNIX and WINDOWS systems)
outfile='log.txt'
)
registerDoParallel(cl)
# PARALLEL LOOP
all_list_nofit <- foreach (file = list_file_names,
.combine=cbind,
.packages = c("ctsmr", "stringr", "lubridate", "zoo", "knitr")) %dopar% {
# List with global parameters
prm <- list()
# Number of threads used by CTSM-R for the estimation computations
prm$threads <- 1
### ---PREPROCESSING------------------------------------------------------------------------------------------------------
## Read the csv file
df <- read.csv(paste(path_in,file,sep=""),sep=";",header=TRUE)
# Get blg uuid from file name
split_file_name <- str_split_fixed(file, "_", 2)
uuid_filtering <- sapply(split_file_name, tail, 1)[[2]]
uuid_filtering <- str_split_fixed(uuid_filtering, ".csv", 2)[1]
# Convert to datetime
df$timedate <- ymd_hms(df$t)
## df$t is now hours since start of the experiment.
df$t <- seq(0, length(df$t)-1, by=1) / 4  # dt = 15 minutes
# Define input X
X <- df[c("t", "Ps", "Ta", "Thm", "yTi", "timedate")]
# Removing NA values
X = X[complete.cases(X), ]
# Automated check of value initialization from the data set
inl[["Th_ini"]] <- c(X$Thm[1], X$Thm[1], X$Thm[1], 40, 45)
inl[["Th_ub"]] <- c(90, 90, 90, 90, 90)
if (X$yTi[1] - X$Ta[1] <= 0) { #testing initial conditions 1.1
inl[["Te_ini"]] <- c(1, 1, 1, 10, 15)
} else if (X$yTi[1] - X$Ta[1] >= 35) { #testing initial conditions 1.2
inl[["Te_ini"]] <- c(15, 15, 15, 10, 15)
} else{
inl[["Te_ini"]] <- c(X$yTi[1] - X$Ta[1], X$yTi[1] - X$Ta[1], X$yTi[1] - X$Ta[1], 10, 15)
}
if ( all(X$Thm < 1) & all(X$Thm > 0) ) { #testing initial conditions 2.1
print('This boiler is binary')
print(uuid_filtering)
inl[["Th_ini"]] <- c(0.1, 0.1, 0.1, 0.1, 0.1)
inl[["Th_ub"]] <- c(1, 1, 1, 1, 1)
} else if (X$Thm[1] < 10) { #testing initial conditions 2.2
inl[["Ti_lb"]] <- c(5, 5, 5, 15, 15)
inl[["Ti_ub"]] <- c(30, 30, 25, 35, 35)
} else if (X$Thm[1] > 35) { #testing initial conditions 2.3
inl[["Ti_lb"]] <- c(10, 10, 10, 15, 15)
inl[["Ti_ub"]] <- c(40, 40, 40, 45, 45)
} else {
inl[["Ti_lb"]] <- c(10, 10, 10, 15, 15)
inl[["Ti_ub"]] <- c(30, 30, 25, 35, 35)
}
### ---- GREYBOX MODEL FITTING ------------------------------------------------------------
while_condition <- TRUE
ite <- "0"
### Model selection
while (while_condition) {
no_fit_condition <- TRUE
# Structure to save max loglikelihood
loglik_ite <- c()
### Looping over models in each iteration
for (m in RC_iterations[[ite]]) { # m - model loop
# Select model to fit
RC_model_considered <- RC_models[[m]]
# Looping over multiple initial values
no_ini_fit <- TRUE
i <- 1
while (no_ini_fit) {
# Parameter dictionary (list)
input_param <- input_ini(X, inl, i)
## ----executeTiTe,results="hide"------------------------------------------
try(fited_model <- RC_model_considered(X, input_param))
# Results extraction - appending loglik_ite list with the current model's log-likelihood
try(loglik_ite <- c(loglik_ite, fited_model$loglik))
# Saving best current model - if its loglikelihood is the maximum observed yet
try(
if (fited_model$loglik == max(loglik_ite)) { # max likelyhood test
fited_model$model_name <- m
best_fit <- fited_model
best_model_name <- m
no_fit_condition <- FALSE
})
# Break out of initial condition loop if one of them suceeds in fitting
try( if (!is.null(fited_model$loglik)) {no_ini_fit <- FALSE})
if (i < 5) {
i <- i + 1
} else {
no_ini_fit <- FALSE
}
} # End of [ite][model][initial values] fitting loop
} # End of [ite][model] fitting loop
# Model did not converge
if(no_fit_condition){
while_condition <- FALSE
return(uuid_filtering)
# Model did converge - 1st ite save
} else if (ite == "0") {
# Update the model list to loop over for next iteration
ite <- best_model_name
best_fit$best_path <- c(best_model_name)
# Calculate the one-step predictions of the state (i.e. the residuals)
tmp <- predict(best_fit)[[1]]
# Calculate the residuals and put them with the data in a data.frame X
X$residuals <- X$yTi - tmp$output$pred$yTi
# nCPBES calculation
nCPBES <- nCPBE_calc(X$residuals)
nCPBES <- tail(nCPBES, n=1)
# Save nCPBES information
best_fit$nCPBES <- c(nCPBES)
# Simply save the best model (max likelyhood selection)
save(best_fit, file=paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
# Model did converge - best ite save
} else {
# Saving the current best model from this iteration
best_fit_new <- best_fit
# Comparing this model with the previous one
load(paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
## Calculate lambda
chisqStat <- -2 * (max(best_fit$loglik) - max(best_fit_new$loglik))
## If this gives a p-value smaller than confidence limit, i.e. 5\%, then the
## new model is significantly better than the previous one
prmDiff <- best_fit_new$model$NPARAM - best_fit$model$NPARAM
## Calculating the p-value of the test
p_val <- 1 - pchisq(chisqStat, prmDiff)
if (p_val < 0.05) { # If the p-value shows significance - we save the new model
# New model is significantly better than the last
# Saving best model
best_newpath <- c(best_fit$best_path, best_model_name)
# Saving best nCPBES
old_nCPBES <- best_fit$nCPBES
best_fit <- best_fit_new  # changing the naming convention of the best_fit model
# Save path information
best_fit$best_path <- best_newpath
# Calculate the one-step predictions of the state (i.e. the residuals)
tmp <- predict(best_fit)[[1]]
# Calculate the residuals and put them with the data in a data.frame X
X$residuals <- X$yTi - tmp$output$pred$yTi
# nCPBES calculation
nCPBES <- nCPBE_calc(X$residuals)
nCPBES <- tail(nCPBES, n=1)
# Save nCPBES information
best_fit$nCPBES <- c(old_nCPBES, nCPBES)
save(best_fit, file=paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
# Update the model list to loop over for next iteration
ite <- best_model_name
} else {
# New model shows no significant improvement of the previous model
# We can terminate the model iteration
while_condition <- FALSE
return(NA)
}
}
} # End of while loop
list_nofit
} # End of [uuid] loop
stopCluster(cl)
### Parallel Looping over building input files
n.cores <- parallel::detectCores() - 1
#create the cluster
cl <- parallel::makeCluster(
n.cores,
type = "PSOCK", #FORK (highly efficient - the main environment doesn’t have to be copied - only available on UNIX), PSOCK (environment of the director needs to be copied - available for both UNIX and WINDOWS systems)
outfile='log.txt'
)
registerDoParallel(cl)
# Export environment variables to cluster - https://www.r-bloggers.com/2015/02/how-to-go-parallel-in-r-basics-tips/
clusterExport(cl, "prm", "RC_models", "RC_iterations", "inl")
stopCluster(cl)
### Parallel Looping over building input files
n.cores <- parallel::detectCores() - 1
#create the cluster
cl <- parallel::makeCluster(
n.cores,
type = "PSOCK", #FORK (highly efficient - the main environment doesn’t have to be copied - only available on UNIX), PSOCK (environment of the director needs to be copied - available for both UNIX and WINDOWS systems)
outfile='log.txt'
)
registerDoParallel(cl)
# Export environment variables to cluster - https://www.r-bloggers.com/2015/02/how-to-go-parallel-in-r-basics-tips/
clusterExport(cl, c("prm", "RC_models", "RC_iterations", "inl"))
# List with global parameters
prm <- list()
# Number of threads used by CTSM-R for the estimation computations
prm$threads <- 1
stopCluster(cl)
# List with global parameters
prm <- list()
# Number of threads used by CTSM-R for the estimation computations
prm$threads <- 1
### Parallel Looping over building input files
n.cores <- parallel::detectCores() - 1
#create the cluster
cl <- parallel::makeCluster(
n.cores,
type = "PSOCK", #FORK (highly efficient - the main environment doesn’t have to be copied - only available on UNIX), PSOCK (environment of the director needs to be copied - available for both UNIX and WINDOWS systems)
outfile='log.txt'
)
registerDoParallel(cl)
# Export environment variables to cluster - https://www.r-bloggers.com/2015/02/how-to-go-parallel-in-r-basics-tips/
clusterExport(cl, c("prm", "RC_models", "RC_iterations", "inl"))
# PARALLEL LOOP
all_list_nofit <- foreach (file = list_file_names,
.combine=cbind,
.packages = c("ctsmr", "stringr", "lubridate", "zoo", "knitr")) %dopar% {
### ---PREPROCESSING------------------------------------------------------------------------------------------------------
## Read the csv file
df <- read.csv(paste(path_in,file,sep=""),sep=";",header=TRUE)
# Get blg uuid from file name
split_file_name <- str_split_fixed(file, "_", 2)
uuid_filtering <- sapply(split_file_name, tail, 1)[[2]]
uuid_filtering <- str_split_fixed(uuid_filtering, ".csv", 2)[1]
# Convert to datetime
df$timedate <- ymd_hms(df$t)
## df$t is now hours since start of the experiment.
df$t <- seq(0, length(df$t)-1, by=1) / 4  # dt = 15 minutes
# Define input X
X <- df[c("t", "Ps", "Ta", "Thm", "yTi", "timedate")]
# Removing NA values
X = X[complete.cases(X), ]
# Automated check of value initialization from the data set
inl[["Th_ini"]] <- c(X$Thm[1], X$Thm[1], X$Thm[1], 40, 45)
inl[["Th_ub"]] <- c(90, 90, 90, 90, 90)
if (X$yTi[1] - X$Ta[1] <= 0) { #testing initial conditions 1.1
inl[["Te_ini"]] <- c(1, 1, 1, 10, 15)
} else if (X$yTi[1] - X$Ta[1] >= 35) { #testing initial conditions 1.2
inl[["Te_ini"]] <- c(15, 15, 15, 10, 15)
} else{
inl[["Te_ini"]] <- c(X$yTi[1] - X$Ta[1], X$yTi[1] - X$Ta[1], X$yTi[1] - X$Ta[1], 10, 15)
}
if ( all(X$Thm < 1) & all(X$Thm > 0) ) { #testing initial conditions 2.1
print('This boiler is binary')
print(uuid_filtering)
inl[["Th_ini"]] <- c(0.1, 0.1, 0.1, 0.1, 0.1)
inl[["Th_ub"]] <- c(1, 1, 1, 1, 1)
} else if (X$Thm[1] < 10) { #testing initial conditions 2.2
inl[["Ti_lb"]] <- c(5, 5, 5, 15, 15)
inl[["Ti_ub"]] <- c(30, 30, 25, 35, 35)
} else if (X$Thm[1] > 35) { #testing initial conditions 2.3
inl[["Ti_lb"]] <- c(10, 10, 10, 15, 15)
inl[["Ti_ub"]] <- c(40, 40, 40, 45, 45)
} else {
inl[["Ti_lb"]] <- c(10, 10, 10, 15, 15)
inl[["Ti_ub"]] <- c(30, 30, 25, 35, 35)
}
### ---- GREYBOX MODEL FITTING ------------------------------------------------------------
while_condition <- TRUE
ite <- "0"
### Model selection
while (while_condition) {
no_fit_condition <- TRUE
# Structure to save max loglikelihood
loglik_ite <- c()
### Looping over models in each iteration
for (m in RC_iterations[[ite]]) { # m - model loop
# Select model to fit
RC_model_considered <- RC_models[[m]]
# Looping over multiple initial values
no_ini_fit <- TRUE
i <- 1
while (no_ini_fit) {
# Parameter dictionary (list)
input_param <- input_ini(X, inl, i)
## ----executeTiTe,results="hide"------------------------------------------
try(fited_model <- RC_model_considered(X, input_param))
# Results extraction - appending loglik_ite list with the current model's log-likelihood
try(loglik_ite <- c(loglik_ite, fited_model$loglik))
# Saving best current model - if its loglikelihood is the maximum observed yet
try(
if (fited_model$loglik == max(loglik_ite)) { # max likelyhood test
fited_model$model_name <- m
best_fit <- fited_model
best_model_name <- m
no_fit_condition <- FALSE
})
# Break out of initial condition loop if one of them suceeds in fitting
try( if (!is.null(fited_model$loglik)) {no_ini_fit <- FALSE})
if (i < 5) {
i <- i + 1
} else {
no_ini_fit <- FALSE
}
} # End of [ite][model][initial values] fitting loop
} # End of [ite][model] fitting loop
# Model did not converge
if(no_fit_condition){
while_condition <- FALSE
return(uuid_filtering)
# Model did converge - 1st ite save
} else if (ite == "0") {
# Update the model list to loop over for next iteration
ite <- best_model_name
best_fit$best_path <- c(best_model_name)
# Calculate the one-step predictions of the state (i.e. the residuals)
tmp <- predict(best_fit)[[1]]
# Calculate the residuals and put them with the data in a data.frame X
X$residuals <- X$yTi - tmp$output$pred$yTi
# nCPBES calculation
nCPBES <- nCPBE_calc(X$residuals)
nCPBES <- tail(nCPBES, n=1)
# Save nCPBES information
best_fit$nCPBES <- c(nCPBES)
# Simply save the best model (max likelyhood selection)
save(best_fit, file=paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
# Model did converge - best ite save
} else {
# Saving the current best model from this iteration
best_fit_new <- best_fit
# Comparing this model with the previous one
load(paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
## Calculate lambda
chisqStat <- -2 * (max(best_fit$loglik) - max(best_fit_new$loglik))
## If this gives a p-value smaller than confidence limit, i.e. 5\%, then the
## new model is significantly better than the previous one
prmDiff <- best_fit_new$model$NPARAM - best_fit$model$NPARAM
## Calculating the p-value of the test
p_val <- 1 - pchisq(chisqStat, prmDiff)
if (p_val < 0.05) { # If the p-value shows significance - we save the new model
# New model is significantly better than the last
# Saving best model
best_newpath <- c(best_fit$best_path, best_model_name)
# Saving best nCPBES
old_nCPBES <- best_fit$nCPBES
best_fit <- best_fit_new  # changing the naming convention of the best_fit model
# Save path information
best_fit$best_path <- best_newpath
# Calculate the one-step predictions of the state (i.e. the residuals)
tmp <- predict(best_fit)[[1]]
# Calculate the residuals and put them with the data in a data.frame X
X$residuals <- X$yTi - tmp$output$pred$yTi
# nCPBES calculation
nCPBES <- nCPBE_calc(X$residuals)
nCPBES <- tail(nCPBES, n=1)
# Save nCPBES information
best_fit$nCPBES <- c(old_nCPBES, nCPBES)
save(best_fit, file=paste(path_out,toString(uuid_filtering),'_fitp.rda', sep=""))
# Update the model list to loop over for next iteration
ite <- best_model_name
} else {
# New model shows no significant improvement of the previous model
# We can terminate the model iteration
while_condition <- FALSE
return(NA)
}
}
} # End of while loop
list_nofit
} # End of [uuid] loop
stopCluster(cl)
all_list_nofit
save(all_list_nofit, file=paste(path_out,'all_list_nofitp.csv', sep=""))
library(cstmr)
library(stringr)
library(lubridate)
library(scales)
# Define paths
root_directory = "C:/Users/20190285/Documents/GitHub/fiftyshadesofgrey"
path_out = paste(root_directory, "/data/out/", sep="")
path_in = paste(root_directory, "/data/in/", sep="")
# Set the working directory. Change this to the location of the example on the computer. Note that "/" is always used in R, also in Windows
setwd(paste(root_directory, "/src", sep=""))
# Source the scripts with functions in the "src" folder. Just a neat way of arranging helping functions in R
source("allmodels.R")
source("utils.R")
# Get all input files names
setwd(path_out)
list_file_names <- list.files(pattern = "*_fitp.rda")
## ------------------------------------------------- Model selection post-processing -------------------------------------------------
# Saving dataframe initialization
R_coefs = c('Ria', 'Rie', 'Rea', 'Rih', 'Rim', 'Ris')
# Saving dataframe initialization
col_names <- c("uuid", "model_name", "HTC", "nCPBES", "iteration", "significance_prop",
"Ti0" , "Tm0" , "Te0" , "Th0" , "Ts0" ,
"Ci" , "Cm" , "Ce" , "Ch" , "Cs" , "Rie", "Rea", "Ria", "Rim", "Rih", "Ris", "Rh" ,
"Aw" , "Ae" ,
"p11", "p22", "p33", "p44", "p55", "e11",
"Ti0_p" , "Tm0_p" , "Te0_p" , "Th0_p" , "Ts0_p" , "Ci_p" , "Cm_p" , "Ce_p" , "Ch_p" , "Cs_p" ,
"Rie_p", "Rea_p", "Ria_p", "Rim_p", "Rih_p", "Ris_p", "Rh_p" ,
"Aw_p" , "Ae_p" ,
"p11_p", "p22_p", "p33_p", "p44_p", "p55_p", "e11_p")
col_names2 <- c("uuid", "ite1", "ite2", "ite3", "ite4", "ite5", "ite6")
# Initialize dataframes
# Results
df_res <- data.frame(matrix(ncol = length(col_names), nrow = 0))
colnames(df_res) <- col_names
# Best path
df_paths <- data.frame(matrix(ncol = length(col_names2), nrow = 0))
colnames(df_paths) <- col_names2
# Best nCPBES
df_nCPBES <- data.frame(matrix(ncol = length(col_names2), nrow = 0))
colnames(df_nCPBES) <- col_names2
# Estimated coefficient p-values, for significance evaluation
coef_pvals <- c("Ti0_p" , "Tm0_p" , "Te0_p" , "Th0_p" , "Ts0_p" ,
"Ci_p" , "Cm_p" , "Ce_p" , "Ch_p" , "Cs_p" ,
"Rie_p", "Rea_p", "Ria_p", "Rim_p", "Rih_p", "Ris_p", "Rh_p" ,
"Aw_p" , "Ae_p" ,
"p11_p", "p22_p", "p33_p", "p44_p", "p55_p", "e11_p")
### Looping over building model files
counts <- 0
list_badoutputs <- c()
for (file in list_file_names){
# --- PREPROCESSING ---
# Read the csv file
load(file)
# Get blg uuid from file name
split_file_name <- str_split_fixed(file, "_", 3)
uuid_filtering <- sapply(split_file_name, tail, 1)[[1]]
# Testing if the result evaluated is correct
if (length(best_fit) == 1) {
list_badoutputs <- c(list_badoutputs, file)
next
} else {
counts <- counts + 1
}
# Extracting model result
val <- summary(best_fit)
# Identifying R coefs from results
coefs <- val$coefficients[1:(length(val$coefficients)/4),1]
coefs_p <- val$coefficients[1:(length(val$coefficients)/4),4]
R_coefs_considered <- Reduce(intersect,list(R_coefs,names(coefs)))
# Calculating HTC in function of available R coefficients
if ("Ria" %in% R_coefs_considered & "Rie" %in% R_coefs_considered & "Rea" %in% R_coefs_considered){
HTC <- 1/(coefs[['Rie']] + coefs[['Rea']]) + 1/coefs[['Ria']] # [kW/C]
} else if ("Rie" %in% R_coefs_considered & "Rea" %in% R_coefs_considered) {
HTC <- 1/(coefs[['Rie']] + coefs[['Rea']]) # [kW/C]
} else {
HTC <- 1/coefs[['Ria']] # [kW/C]
}
# Saving HTC value
df_res[[counts, 3]] <- HTC
# Save coefficients and their respective p-values
for (i in 1:length(names(coefs))){
df_res[[counts,  names(coefs)[i]]] <- coefs[[names(coefs)[i]]]        # parameter estimates
df_res[[counts,  paste(names(coefs_p)[i], "_p", sep="")]] <- coefs_p[[names(coefs_p)[i]]]  # parameter p-values
}
# Save uuid, model name
df_res[[counts, 1]] <- uuid_filtering
df_res[[counts, 2]] <- best_fit$model_name
# Save model selection path
df_paths[[counts, 1]] <- uuid_filtering
for (i in 1:length(best_fit$best_path)){
df_paths[[counts, i+1]] <- best_fit$best_path[i]
}
# Save model nCPBES
df_nCPBES[[counts, 1]] <- uuid_filtering
for (i in 1:length(best_fit$nCPBES)){
df_nCPBES[[counts, i+1]] <- best_fit$nCPBES[i]
}
### Save normalized Cumulated Periodogram Boundary Excess Sum
df_res[[counts, 4]] <- tail(best_fit$nCPBES, n=1)
# Saving Model Iteration
df_res[[counts, 5]] <- length(best_fit$best_path)
# Significance of estimated parameter - proportion: significance_prop
signifiance <- c()
for (c in coef_pvals){
signifiance <- c(signifiance, df_res[[counts, c]] < 0.05)
}
significance_prop <- sum(signifiance[!is.na(signifiance)]*1)/length(signifiance[!is.na(signifiance)])*100
df_res[[counts, 6]] <- significance_prop
}
# Save dataframe as csv file/ spark table
write.csv(df_res, paste(path_out,'all_final_fitsp.csv', sep=""), row.names = TRUE)
write.csv(df_paths, paste(path_out,'all_model_pathsp.csv', sep=""), row.names = TRUE)
write.csv(df_nCPBES, paste(path_out,'all_nCPBESp.csv', sep=""), row.names = TRUE)
#write.csv(list_badoutputs, paste(path_out,'list_badoutputs.csv', sep=""))
